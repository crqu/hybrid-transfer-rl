{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rlberry.agents import AgentWithSimplePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import rlberry.spaces as spaces\n",
    "from rlberry_research.envs.finite import GridWorld\n",
    "from rlberry_research.rendering import Scene, GeometricPrimitive\n",
    "\n",
    "import rlberry\n",
    "\n",
    "logger = rlberry.logger\n",
    "\n",
    "\n",
    "# def get_nroom_state_coord(state_index, nroom_env):\n",
    "#     yy, xx = nroom_env.index2coord[state_index]\n",
    "#     # centering\n",
    "#     xx = xx + 0.5\n",
    "#     yy = yy + 0.5\n",
    "#     # map to [0, 1]\n",
    "#     xx = xx / nroom_env.ncols\n",
    "#     yy = yy / nroom_env.nrows\n",
    "#     return np.array([xx, yy])\n",
    "#     2024/08/25: modified\n",
    "#     make the trap states random\n",
    "\n",
    "\n",
    "class NRoom(GridWorld):\n",
    "    \"\"\"\n",
    "    GridWorld with N rooms of size L x L. The agent starts in the middle room.\n",
    "\n",
    "    There is one small and easy reward in the first room,\n",
    "    one big reward in the last room and zero reward elsewhere.\n",
    "\n",
    "    There is a 5% error probability in the transitions when taking an action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nrooms : int\n",
    "        Number of rooms.\n",
    "    reward_free : bool, default=False\n",
    "        If true, no rewards are given to the agent.\n",
    "    array_observation:\n",
    "        If true, the observations are converted to an array (x, y)\n",
    "        instead of a discrete index.\n",
    "        The underlying discrete space is saved in env.discrete_observation_space.\n",
    "    room_size : int\n",
    "        Dimension (L) of each room (L x L).\n",
    "    success_probability : double, default: 0.95\n",
    "        Sucess probability of an action. A failure is going to the wrong direction.\n",
    "    remove_walls : bool, default: False\n",
    "        If True, remove walls. Useful for debug.\n",
    "    initial_state_distribution: {'center', 'uniform'}\n",
    "        If 'center', always start at the center.\n",
    "        If 'uniform', start anywhere with uniform probability.\n",
    "    include_traps: bool, default: False\n",
    "        If true, each room will have a terminal state (a \"trap\").\n",
    "    Notes\n",
    "    -----\n",
    "    The function env.sample() does not handle conversions to array states\n",
    "    when array_observation is True. Only the functions env.reset() and\n",
    "    env.step() are covered.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"N-Room\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nrooms=7,\n",
    "        reward_free=False,\n",
    "        array_observation=False,\n",
    "        room_size=5,\n",
    "        success_probability=0.95,\n",
    "        remove_walls=False,\n",
    "        initial_state_distribution=\"center\",\n",
    "        include_traps=False,\n",
    "    ):\n",
    "        assert nrooms > 0, \"nrooms must be > 0\"\n",
    "        assert initial_state_distribution in (\"center\", \"uniform\")\n",
    "\n",
    "        self.reward_free = reward_free\n",
    "        self.array_observation = array_observation\n",
    "        self.nrooms = nrooms\n",
    "        self.room_size = room_size\n",
    "        self.success_probability = success_probability\n",
    "        self.remove_walls = remove_walls\n",
    "        self.initial_state_distribution = initial_state_distribution\n",
    "        self.include_traps = include_traps\n",
    "\n",
    "        # Max number of rooms/columns per row\n",
    "        self.max_rooms_per_row = 5\n",
    "\n",
    "        # Room size (default = 5x5)\n",
    "        self.room_size = room_size\n",
    "\n",
    "        # Grid size\n",
    "        self.room_nrows = math.ceil(nrooms / self.max_rooms_per_row)\n",
    "        if self.room_nrows > 1:\n",
    "            self.room_ncols = self.max_rooms_per_row\n",
    "        else:\n",
    "            self.room_ncols = nrooms\n",
    "        nrows = self.room_size * self.room_nrows + (self.room_nrows - 1)\n",
    "        ncols = self.room_size * self.room_ncols + (self.room_ncols - 1)\n",
    "\n",
    "        # # walls\n",
    "        walls = []\n",
    "        for room_col in range(self.room_ncols - 1):\n",
    "            col = (room_col + 1) * (self.room_size + 1) - 1\n",
    "            for jj in range(nrows):\n",
    "                if (jj % (self.room_size + 1)) != (self.room_size // 2):\n",
    "                    walls.append((jj, col))\n",
    "\n",
    "        for room_row in range(self.room_nrows - 1):\n",
    "            row = (room_row + 1) * (self.room_size + 1) - 1\n",
    "            for jj in range(ncols):\n",
    "                walls.append((row, jj))\n",
    "\n",
    "        # process each room\n",
    "        start_coord = None\n",
    "        terminal_state = None\n",
    "        self.traps = []\n",
    "        count = 0\n",
    "        for room_r in range(self.room_nrows): \n",
    "            if room_r % 2 == 0:\n",
    "                cols_iterator = range(self.room_ncols)\n",
    "            else:\n",
    "                cols_iterator = reversed(range(self.room_ncols))\n",
    "            for room_c in cols_iterator:\n",
    "                # existing rooms\n",
    "                if count < self.nrooms:\n",
    "                    # remove top wall\n",
    "                    if ((room_c == self.room_ncols - 1) and (room_r % 2 == 0)) or (\n",
    "                        (room_c == 0) and (room_r % 2 == 1)\n",
    "                    ):\n",
    "                        if room_r != self.room_nrows - 1:\n",
    "                            wall_to_remove = self._convert_room_coord_to_global(\n",
    "                                room_r, room_c, self.room_size, self.room_size // 2\n",
    "                            )\n",
    "                            if wall_to_remove in walls:\n",
    "                                walls.remove(wall_to_remove)\n",
    "                # rooms to remove\n",
    "                else:\n",
    "                    for ii in range(-1, self.room_size + 1):\n",
    "                        for jj in range(-1, self.room_size + 1):\n",
    "                            wall_to_include = self._convert_room_coord_to_global(\n",
    "                                room_r, room_c, ii, jj\n",
    "                            )\n",
    "                            if (\n",
    "                                wall_to_include[0] >= 0\n",
    "                                and wall_to_include[0] < nrows\n",
    "                                and wall_to_include[1] >= 0\n",
    "                                and wall_to_include[1] < ncols\n",
    "                                and (wall_to_include not in walls)\n",
    "                            ):\n",
    "                                walls.append(wall_to_include)\n",
    "                    pass\n",
    "\n",
    "                # start coord\n",
    "                if count == nrooms // 2:\n",
    "                    start_coord = self._convert_room_coord_to_global(\n",
    "                        room_r, room_c, self.room_size // 2-1, self.room_size // 2-1\n",
    "                    )\n",
    "                # terminal state\n",
    "                if count == nrooms - 1:\n",
    "                    terminal_state = self._convert_room_coord_to_global(\n",
    "                        room_r, room_c, self.room_size // 2+1, self.room_size // 2+1\n",
    "                    )\n",
    "                # trap\n",
    "                if include_traps:\n",
    "                    self.traps.append(\n",
    "                        self._convert_room_coord_to_global(\n",
    "                            room_r,\n",
    "                            room_c,\n",
    "                            self.room_size // 2 ,\n",
    "                            self.room_size // 2 -1,\n",
    "                        )\n",
    "                    )\n",
    "                    self.traps.append(\n",
    "                        self._convert_room_coord_to_global(\n",
    "                            room_r,\n",
    "                            room_c,\n",
    "                            self.room_size // 2 -1 ,\n",
    "                            self.room_size // 2 ,\n",
    "                        )\n",
    "                    )\n",
    "                    self.traps.append(\n",
    "                        self._convert_room_coord_to_global(\n",
    "                            room_r,\n",
    "                            room_c,\n",
    "                            self.room_size // 2,\n",
    "                            self.room_size -1 ,\n",
    "                        )\n",
    "                    )\n",
    "                count += 1\n",
    "\n",
    "        terminal_states = (terminal_state,) + tuple(self.traps)\n",
    "\n",
    "        if self.reward_free:\n",
    "            reward_at = {}\n",
    "        else:\n",
    "            reward_at = {\n",
    "                terminal_state: 1.0,\n",
    "                start_coord: 0.01,\n",
    "                (self.room_size // 2, self.room_size // 2): 0.1,\n",
    "                (1,self.room_size -1):0.15\n",
    "            }\n",
    "\n",
    "        # Check remove_walls\n",
    "        if remove_walls:\n",
    "            walls = ()\n",
    "\n",
    "        # Init base class\n",
    "        GridWorld.__init__(\n",
    "            self,\n",
    "            nrows=nrows,\n",
    "            ncols=ncols,\n",
    "            start_coord=start_coord,\n",
    "            terminal_states=terminal_states,\n",
    "            success_probability=success_probability,\n",
    "            reward_at=reward_at,\n",
    "            walls=walls,\n",
    "            default_reward=0.0,\n",
    "        )\n",
    "\n",
    "        # Check initial distribution\n",
    "        if initial_state_distribution == \"uniform\":\n",
    "            distr = np.ones(self.observation_space.n) / self.observation_space.n\n",
    "            self.set_initial_state_distribution(distr)\n",
    "\n",
    "        # spaces\n",
    "        if self.array_observation:\n",
    "            self.discrete_observation_space = self.observation_space\n",
    "            self.observation_space = spaces.Box(0.0, 1.0, shape=(2,))\n",
    "\n",
    "    def _convert_room_coord_to_global(\n",
    "        self, room_row, room_col, room_coord_row, room_coord_col\n",
    "    ):\n",
    "        col_offset = (self.room_size + 1) * room_col\n",
    "        row_offset = (self.room_size + 1) * room_row\n",
    "\n",
    "        row = room_coord_row + row_offset\n",
    "        col = room_coord_col + col_offset\n",
    "        return (row, col)\n",
    "\n",
    "    def _convert_index_to_float_coord(self, state_index):\n",
    "        yy, xx = self.index2coord[state_index]\n",
    "\n",
    "        # centering\n",
    "        xx = xx + 0.5\n",
    "        yy = yy + 0.5\n",
    "        # map to [0, 1]\n",
    "        xx = xx / self.ncols\n",
    "        yy = yy / self.nrows\n",
    "        return np.array([xx, yy])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.state, info = GridWorld.reset(self, seed=seed, options=options)\n",
    "        state_to_return = self.state\n",
    "        if self.array_observation:\n",
    "            state_to_return = self._convert_index_to_float_coord(self.state)\n",
    "        return state_to_return, info\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"Invalid action!\"\n",
    "\n",
    "        # save state for rendering\n",
    "        if self.is_render_enabled():\n",
    "            self.append_state_for_rendering(self.state)\n",
    "\n",
    "        # take step\n",
    "        next_state, reward, terminated, truncated, info = self.sample(\n",
    "            self.state, action\n",
    "        )\n",
    "        self.state = next_state\n",
    "\n",
    "        state_to_return = self.state\n",
    "        if self.array_observation:\n",
    "            state_to_return = self._convert_index_to_float_coord(self.state)\n",
    "\n",
    "        return state_to_return, reward, terminated, truncated, info\n",
    "\n",
    "    def get_background(self):\n",
    "        \"\"\"\n",
    "        Returne a scene (list of shapes) representing the background\n",
    "        \"\"\"\n",
    "        bg = Scene()\n",
    "\n",
    "        # traps\n",
    "        for y, x in self.traps:\n",
    "            shape = GeometricPrimitive(\"POLYGON\")\n",
    "            shape.set_color((0.5, 0.0, 0.0))\n",
    "            shape.add_vertex((x, y))\n",
    "            shape.add_vertex((x + 1, y))\n",
    "            shape.add_vertex((x + 1, y + 1))\n",
    "            shape.add_vertex((x, y + 1))\n",
    "            bg.add_shape(shape)\n",
    "\n",
    "        # walls\n",
    "        for wall in self.walls:\n",
    "            y, x = wall\n",
    "            shape = GeometricPrimitive(\"POLYGON\")\n",
    "            shape.set_color((0.25, 0.25, 0.25))\n",
    "            shape.add_vertex((x, y))\n",
    "            shape.add_vertex((x + 1, y))\n",
    "            shape.add_vertex((x + 1, y + 1))\n",
    "            shape.add_vertex((x, y + 1))\n",
    "            bg.add_shape(shape)\n",
    "\n",
    "        # rewards\n",
    "        for y, x in self.reward_at:\n",
    "            flag = GeometricPrimitive(\"POLYGON\")\n",
    "            rwd = self.reward_at[(y, x)]\n",
    "            if rwd == 1.0:\n",
    "                flag.set_color((0.0, 0.5, 0.0))\n",
    "            elif rwd == 0.1:\n",
    "                flag.set_color((0.0, 0.0, 0.5))\n",
    "            else:\n",
    "                flag.set_color((0.5, 0.0, 0.0))\n",
    "\n",
    "            x += 0.5\n",
    "            y += 0.25\n",
    "            flag.add_vertex((x, y))\n",
    "            flag.add_vertex((x + 0.25, y + 0.5))\n",
    "            flag.add_vertex((x - 0.25, y + 0.5))\n",
    "            bg.add_shape(flag)\n",
    "\n",
    "        return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RF-agent\n",
    "class RFAgent(AgentWithSimplePolicy):\n",
    "    name=\"RFAgent\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma=1,\n",
    "            horizon=100,\n",
    "            delta=0.1,\n",
    "            varepsilon=0.1,\n",
    "            cb=0.01,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # init base class\n",
    "        AgentWithSimplePolicy.__init__(self,env=env,**kwargs)\n",
    "\n",
    "        # basic parameters\n",
    "        self.gamma=gamma\n",
    "        self.horizon=horizon\n",
    "        self.varepsilon=varepsilon\n",
    "        self.delta=delta\n",
    "        self.cb=cb\n",
    "        self.initial_state,_=self.env.reset()\n",
    "        self.S=self.env.observation_space.n\n",
    "        self.A=self.env.action_space.n\n",
    "\n",
    "        #task information\n",
    "        self.R_hat=np.zeros((self.S,self.A)) # reward info\n",
    "        self.bonus=np.ones((self.S,self.A))*self.horizon #should multiply H\n",
    "        self.P_hat=np.ones((self.S,self.A,self.S))*1.0/self.S #estimated transition kernel\n",
    "        self.Nsas=np.zeros((self.S,self.A,self.S)) #visitation count of (s,a,s')\n",
    "        self.Nsa=np.zeros((self.S,self.A)) #visitation count of (s,a)\n",
    "\n",
    "        # uncertainty function\n",
    "        self.Whsa=np.ones((self.horizon,self.S,self.A))\n",
    "\n",
    "    def eval(self,**kwargs):\n",
    "        \"\"\"\n",
    "        Returns a value corresponding to the evaluation of the agent on the\n",
    "        evaluation environment.\n",
    "\n",
    "        For instance, it can be a Monte-Carlo evaluation of the policy learned in fit().\n",
    "        \"\"\"\n",
    "        return super().eval() #use the eval() from AgentWithSimplePolicy\n",
    "    \n",
    "    #calculate bonus\n",
    "    def beta(self,n):\n",
    "        beta = np.log(6*self.S*self.A*self.horizon/self.delta) + self.S*np.log(8*np.exp(1)*(n+1))\n",
    "        return beta\n",
    "    \n",
    "    def update(self,s,a,next_state,reward):\n",
    "        self.Nsas[s,a,next_state]+=1\n",
    "        self.Nsa[s,a]+=1\n",
    "\n",
    "        n_sa=self.Nsa[s,a]\n",
    "        n_sas=self.Nsas[s,a,:]\n",
    "        self.P_hat[s,a,:]=n_sas/n_sa\n",
    "        self.R_hat[s,a]=reward\n",
    "        self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "\n",
    "    def backW(self,s,a,h):\n",
    "        out=0\n",
    "        for i in range(self.S):\n",
    "            out+=self.P_hat[s,a,i]*np.max(self.Whsa[h+1,i,:])\n",
    "        return out\n",
    "\n",
    "    def update_value(self):\n",
    "        for h in range(self.horizon-1,-1,-1):\n",
    "            if h==self.horizon-1:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        self.Whsa[h,s,a]=min(1,0.0005*self.cb*4*self.horizon*self.bonus[s,a])\n",
    "            else:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        self.Whsa[h,s,a]=min(1,0.0005*self.cb*4*self.horizon*self.bonus[s,a]+self.backW(s,a,h))\n",
    "\n",
    "    def policy(self,observation,step):\n",
    "        return self.Whsa[step,observation,:].argmax()\n",
    "    \n",
    "    def fit(self,budget,**kwargs):\n",
    "        T=budget\n",
    "        for t in range(T):\n",
    "            if ((t+1)%1000)==0:\n",
    "                print(\"Episode\",t+1)\n",
    "            self.update_value()\n",
    "            observation,info=self.env.reset()\n",
    "            done=False\n",
    "            step=0\n",
    "            reward=0\n",
    "            current_state=0\n",
    "            while step<self.horizon:\n",
    "                # terminal state is an absorbing state\n",
    "                if done:\n",
    "                    action=self.policy(current_state,step)\n",
    "                    next_step=current_state\n",
    "                    self.update(current_state,action,next_step,reward)\n",
    "                else:\n",
    "                    action=self.policy(observation,step)\n",
    "                    next_step,reward,terminated,truncated,info=self.env.step(action)\n",
    "                    #update visitation count and policy\n",
    "                    self.update(observation,action,next_step,reward)\n",
    "                    current_state=observation\n",
    "                    observation=next_step\n",
    "                    done=terminated or truncated\n",
    "                step+=1\n",
    "        \n",
    "        np.save(\"source_counts.npy\",self.Nsa)\n",
    "        np.save(\"source_transition.npy\",self.P_hat)\n",
    "        return self.R_hat,self.P_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate source data\n",
    "source_env=NRoom(\n",
    "    nrooms=1,\n",
    "    remove_walls=False,\n",
    "    room_size=4,\n",
    "    initial_state_distribution=\"center\", #see if we can change to others\n",
    "    include_traps=False,\n",
    ")\n",
    "T=100000\n",
    "agent=RFAgent(source_env,gamma=1,horizon=20,cb=0.002)\n",
    "agent.fit(budget=T)\n",
    "# save source_count and source_transition after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get V_opt by value iteration\n",
    "def value_iteration(S,A,H,R,P):\n",
    "    # initialize value function\n",
    "    V=np.zeros((H,S))\n",
    "    # value iteration\n",
    "    for h in range(H-1,-1,-1):\n",
    "        if h==H-1:\n",
    "            for s in range(S):\n",
    "                V[h,s]=np.max(R[s,:])\n",
    "        else:\n",
    "            for s in range(S):\n",
    "                if (sum([1-P[s,a,s] for a in range(A)])<=0.01):\n",
    "                    V[h,s]=np.max(R[s,:])\n",
    "                else:\n",
    "                    best_next_value=0\n",
    "                    for a in range(A):\n",
    "                        current_next_value=R[s,a]+np.dot(P[s,a,:],V[h+1,:])\n",
    "                        if current_next_value>best_next_value:\n",
    "                            best_next_value=current_next_value\n",
    "                    V[h,s]=best_next_value\n",
    "    # return V_1, dim=S\n",
    "    return V[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate v_opt for different success probability\n",
    "opt_success=[]\n",
    "for i in range(1,13):\n",
    "    success=0.95-i*0.05\n",
    "    # generate estimate of r and p\n",
    "    env=NRoom(\n",
    "        nrooms=1,\n",
    "        remove_walls=False,\n",
    "        room_size=4,\n",
    "        initial_state_distribution=\"center\", #see if we can change to others\n",
    "        include_traps=True,\n",
    "        success_probability=success\n",
    "    )\n",
    "    rfAgent=RFAgent(env=env,horizon=20,gamma=1,cb=0.002)\n",
    "    R,P=rfAgent.fit(budget=10000)\n",
    "    # calculate optimal value\n",
    "    opt=value_iteration(env=env,R=R,P=P)\n",
    "    opt_v=opt[rfAgent.initial_state]\n",
    "    opt_success.append([success,opt_v])\n",
    "opt_success=np.array(opt_success)\n",
    "np.save(\"../data/vopt.npy\",opt_success)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlberry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
